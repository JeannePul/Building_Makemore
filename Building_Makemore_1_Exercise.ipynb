{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS4GJlgIof+Rw8AwkBORvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeannePul/Building_Makemore_in_Youtube/blob/main/Building_Makemore_1_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1"
      ],
      "metadata": {
        "id": "MEa3cC__jrpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
      ],
      "metadata": {
        "id": "8VeoFgZ8jw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "kRwGe4GxkTg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', \"r\" ).read().splitlines()"
      ],
      "metadata": {
        "id": "RQNMBDSUkb1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "yW5fwocLluJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs, ys = [], []\n",
        "\n",
        "for w in words[:1]:\n",
        "  chs =  ['.'] + ['.'] + list(w) + ['.']  # For our model, it is also very important to know what \"Start\" & \"End-characters\" are.\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    # print( ch1, ch2, ch3)\n",
        "    xs.append((ix1,ix2))\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "print(xs[0].shape, xs.shape, ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOy0MhBeklTW",
        "outputId": "0e39efd0-d839-4907-c500-7fbbde41e7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2]) torch.Size([5, 2]) tensor([ 5, 13, 13,  1,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs, num_classes = 27)\n",
        "xenc = xenc.float()\n",
        "\n",
        "\"\"\"\n",
        "How do i deal with 2 inputs? I put them next to each other.\n",
        "For this to work I first have to convert them to a list, so I can append them.\n",
        "\n",
        "Note: I am sure there is a more efficient and easy way to do it...\n",
        "\"\"\"\n",
        "\n",
        "xenc_54 = []\n",
        "for i in range(len(xs)):\n",
        "  a = xenc[i][0].tolist()\n",
        "  b = xenc[i][1].tolist()\n",
        "  ab = a + b\n",
        "  xenc_54.append(ab)\n",
        "\n",
        "xenc_54 = torch.tensor(xenc_54) # -> We get a 54-long input vector, not 27!\n",
        "print(xenc_54)\n",
        "\n",
        "\"\"\"\n",
        "xenc_54 is now the whole input of the first word.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Bz6GD0TplqSJ",
        "outputId": "26ccc11a-d124-485a-d5bf-c663799aec7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nxenc_54 is now the whole input of the first word.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.rand((54,27)) # The dimension of our weights have to match the input!\n",
        "xenc_54 @ W # first pass through our NN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvX2ubWbKBnv",
        "outputId": "9fe7250c-b697-408d-e871-4527fab33b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0126, 1.3068, 1.2511, 1.1278, 0.7323, 1.7860, 1.0280, 1.2079, 0.9577,\n",
              "         1.0034, 1.1422, 0.7177, 0.3020, 1.1308, 1.0603, 0.7632, 1.9147, 0.8420,\n",
              "         1.7797, 0.9094, 0.5670, 1.0212, 1.1307, 0.4419, 0.5611, 0.5193, 1.1965],\n",
              "        [1.1428, 1.5073, 1.6399, 1.5626, 0.8416, 1.8569, 0.3914, 0.8693, 1.8135,\n",
              "         1.2209, 0.6740, 0.2783, 0.6055, 1.4479, 0.7545, 0.7687, 1.2532, 0.9835,\n",
              "         1.8178, 1.1408, 1.2117, 1.3541, 0.9437, 0.5598, 0.4213, 1.1189, 0.5651],\n",
              "        [1.2855, 0.9100, 0.9666, 1.2978, 0.1994, 0.2689, 1.5140, 1.5817, 1.0722,\n",
              "         1.3607, 1.5112, 0.7861, 0.7734, 1.2677, 0.6243, 1.2229, 0.1635, 1.3436,\n",
              "         0.7974, 1.3431, 0.9738, 0.9421, 1.2220, 0.9810, 0.7244, 1.2382, 0.4477],\n",
              "        [0.9086, 0.7706, 0.8170, 0.7797, 0.4709, 0.6074, 0.8985, 1.6370, 0.7162,\n",
              "         1.2440, 1.0091, 1.0744, 1.0407, 1.4227, 0.6049, 0.7674, 0.5781, 1.1583,\n",
              "         0.9407, 0.5896, 1.4844, 0.9898, 1.3896, 0.8830, 1.1487, 1.5050, 0.1635],\n",
              "        [0.6158, 1.4113, 1.0031, 0.9840, 0.5336, 0.9658, 0.0594, 1.2505, 1.0223,\n",
              "         1.2006, 0.9171, 1.2389, 1.4957, 0.8711, 1.1259, 0.6021, 1.4707, 0.7559,\n",
              "         0.9223, 0.2015, 0.7571, 0.4679, 1.2747, 1.2303, 0.9680, 1.2279, 0.9311]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output\n",
        "\n",
        "logits = xenc_54 @ W # log-counts\n",
        "\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True) # = softmax\n",
        "\n",
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYm0DdkMvYLE",
        "outputId": "4d417c83-e865-4995-e8a7-f308062ec192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0341, 0.0458, 0.0433, 0.0383, 0.0258, 0.0740, 0.0347, 0.0415, 0.0323,\n",
              "         0.0338, 0.0388, 0.0254, 0.0168, 0.0384, 0.0358, 0.0266, 0.0841, 0.0288,\n",
              "         0.0735, 0.0308, 0.0219, 0.0344, 0.0384, 0.0193, 0.0217, 0.0208, 0.0410],\n",
              "        [0.0362, 0.0521, 0.0595, 0.0550, 0.0268, 0.0739, 0.0171, 0.0275, 0.0707,\n",
              "         0.0391, 0.0226, 0.0152, 0.0211, 0.0491, 0.0245, 0.0249, 0.0404, 0.0308,\n",
              "         0.0710, 0.0361, 0.0387, 0.0447, 0.0296, 0.0202, 0.0176, 0.0353, 0.0203],\n",
              "        [0.0462, 0.0317, 0.0336, 0.0468, 0.0156, 0.0167, 0.0581, 0.0621, 0.0373,\n",
              "         0.0498, 0.0579, 0.0280, 0.0277, 0.0454, 0.0239, 0.0434, 0.0150, 0.0490,\n",
              "         0.0284, 0.0489, 0.0338, 0.0328, 0.0434, 0.0341, 0.0264, 0.0441, 0.0200],\n",
              "        [0.0335, 0.0292, 0.0306, 0.0295, 0.0216, 0.0248, 0.0332, 0.0695, 0.0277,\n",
              "         0.0469, 0.0371, 0.0396, 0.0383, 0.0561, 0.0248, 0.0291, 0.0241, 0.0431,\n",
              "         0.0346, 0.0244, 0.0596, 0.0364, 0.0542, 0.0327, 0.0426, 0.0609, 0.0159],\n",
              "        [0.0252, 0.0557, 0.0371, 0.0364, 0.0232, 0.0357, 0.0144, 0.0475, 0.0378,\n",
              "         0.0451, 0.0340, 0.0469, 0.0606, 0.0325, 0.0419, 0.0248, 0.0591, 0.0289,\n",
              "         0.0342, 0.0166, 0.0290, 0.0217, 0.0486, 0.0465, 0.0358, 0.0464, 0.0345]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(probs[torch.arange(len(xs)), ys])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FSNeZ0HYKnD",
        "outputId": "2dcae628-32f9-408e-a3ff-dd43f0ecd298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0740, 0.0491, 0.0454, 0.0292, 0.0252])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6TqgGI5Ku5T",
        "outputId": "a73bf6b0-8676-46e7-b031-b22142821adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlls = torch.zeros(len(xs)) # emma provides us with 5 examples\n",
        "for i in range(len(xs)):\n",
        "  # i-th bigram:\n",
        "  x = xs[i].tolist() # input character index\n",
        "  y = ys[i].item() # label character index\n",
        "  print('--------')\n",
        "  print(x, y)\n",
        "  print(f'bigram example {i+1}: {itos[x[0]]}{itos[x[1]]}  ->  {itos[y]} (Indexes ({x[0]},{x[1]}), {y})')\n",
        "  print('input to the neural net:', x, '\\n')\n",
        "  print('output probabilities from the neural net:', probs[i], '\\n')\n",
        "  p = probs[i, y]\n",
        "  print('probability assigned by the net to the the correct character:', p.item(), '\\n')\n",
        "\n",
        "  logp = torch.log(p)\n",
        "  print('log likelihood:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item(), '\\n')\n",
        "  nlls[i] = nll\n",
        "\n",
        "print('=========', '\\n')\n",
        "print('average negative log likelihood, i.e. loss =', nlls.mean().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19V3xxOFK0ys",
        "outputId": "2ca27a60-9345-4caf-9971-92756f3423d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------\n",
            "[0, 0] 5\n",
            "bigram example 1: ..  ->  e (Indexes (0,0), 5)\n",
            "input to the neural net: [0, 0] \n",
            "\n",
            "output probabilities from the neural net: tensor([0.0341, 0.0458, 0.0433, 0.0383, 0.0258, 0.0740, 0.0347, 0.0415, 0.0323,\n",
            "        0.0338, 0.0388, 0.0254, 0.0168, 0.0384, 0.0358, 0.0266, 0.0841, 0.0288,\n",
            "        0.0735, 0.0308, 0.0219, 0.0344, 0.0384, 0.0193, 0.0217, 0.0208, 0.0410]) \n",
            "\n",
            "probability assigned by the net to the the correct character: 0.07395273447036743 \n",
            "\n",
            "log likelihood: -2.6043291091918945\n",
            "negative log likelihood: 2.6043291091918945 \n",
            "\n",
            "--------\n",
            "[0, 5] 13\n",
            "bigram example 2: .e  ->  m (Indexes (0,5), 13)\n",
            "input to the neural net: [0, 5] \n",
            "\n",
            "output probabilities from the neural net: tensor([0.0362, 0.0521, 0.0595, 0.0550, 0.0268, 0.0739, 0.0171, 0.0275, 0.0707,\n",
            "        0.0391, 0.0226, 0.0152, 0.0211, 0.0491, 0.0245, 0.0249, 0.0404, 0.0308,\n",
            "        0.0710, 0.0361, 0.0387, 0.0447, 0.0296, 0.0202, 0.0176, 0.0353, 0.0203]) \n",
            "\n",
            "probability assigned by the net to the the correct character: 0.04906757175922394 \n",
            "\n",
            "log likelihood: -3.014556884765625\n",
            "negative log likelihood: 3.014556884765625 \n",
            "\n",
            "--------\n",
            "[5, 13] 13\n",
            "bigram example 3: em  ->  m (Indexes (5,13), 13)\n",
            "input to the neural net: [5, 13] \n",
            "\n",
            "output probabilities from the neural net: tensor([0.0462, 0.0317, 0.0336, 0.0468, 0.0156, 0.0167, 0.0581, 0.0621, 0.0373,\n",
            "        0.0498, 0.0579, 0.0280, 0.0277, 0.0454, 0.0239, 0.0434, 0.0150, 0.0490,\n",
            "        0.0284, 0.0489, 0.0338, 0.0328, 0.0434, 0.0341, 0.0264, 0.0441, 0.0200]) \n",
            "\n",
            "probability assigned by the net to the the correct character: 0.04538702592253685 \n",
            "\n",
            "log likelihood: -3.092529058456421\n",
            "negative log likelihood: 3.092529058456421 \n",
            "\n",
            "--------\n",
            "[13, 13] 1\n",
            "bigram example 4: mm  ->  a (Indexes (13,13), 1)\n",
            "input to the neural net: [13, 13] \n",
            "\n",
            "output probabilities from the neural net: tensor([0.0335, 0.0292, 0.0306, 0.0295, 0.0216, 0.0248, 0.0332, 0.0695, 0.0277,\n",
            "        0.0469, 0.0371, 0.0396, 0.0383, 0.0561, 0.0248, 0.0291, 0.0241, 0.0431,\n",
            "        0.0346, 0.0244, 0.0596, 0.0364, 0.0542, 0.0327, 0.0426, 0.0609, 0.0159]) \n",
            "\n",
            "probability assigned by the net to the the correct character: 0.029214823618531227 \n",
            "\n",
            "log likelihood: -3.533079147338867\n",
            "negative log likelihood: 3.533079147338867 \n",
            "\n",
            "--------\n",
            "[13, 1] 0\n",
            "bigram example 5: ma  ->  . (Indexes (13,1), 0)\n",
            "input to the neural net: [13, 1] \n",
            "\n",
            "output probabilities from the neural net: tensor([0.0252, 0.0557, 0.0371, 0.0364, 0.0232, 0.0357, 0.0144, 0.0475, 0.0378,\n",
            "        0.0451, 0.0340, 0.0469, 0.0606, 0.0325, 0.0419, 0.0248, 0.0591, 0.0289,\n",
            "        0.0342, 0.0166, 0.0290, 0.0217, 0.0486, 0.0465, 0.0358, 0.0464, 0.0345]) \n",
            "\n",
            "probability assigned by the net to the the correct character: 0.025155669078230858 \n",
            "\n",
            "log likelihood: -3.6826720237731934\n",
            "negative log likelihood: 3.6826720237731934 \n",
            "\n",
            "========= \n",
            "\n",
            "average negative log likelihood, i.e. loss = 3.1854331493377686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "6-3FzcS5OVLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs, ys = [], []\n",
        "\n",
        "for w in words[:]:\n",
        "  chs =  ['.'] + ['.'] + list(w) + ['.']  # For our model, it is also very important to know what \"Start\" & \"End-characters\" are.\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    # print( ch1, ch2, ch3)\n",
        "    xs.append((ix1,ix2))\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num= xs.shape[0]\n",
        "print(xs.shape, ys.shape)\n",
        "print('number of examples: ', num)\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRcbMMQEOYJu",
        "outputId": "bf043bae-d045-4f5a-edbe-03f0c000b70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([228146, 2]) torch.Size([228146])\n",
            "number of examples:  228146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc = F.one_hot(xs, num_classes = 27)\n",
        "xenc = xenc.float()\n",
        "z = xenc.shape[0]\n",
        "\n",
        "xenc_54 = []\n",
        "for i in range(z):\n",
        "  a = xenc[i][0].tolist()\n",
        "  b = xenc[i][1].tolist()\n",
        "  ab = a + b\n",
        "  xenc_54.append(ab)\n",
        "xenc_54 = torch.tensor(xenc_54)\n",
        "\n",
        "for k in range(100):\n",
        "\n",
        "  #forward pass\n",
        "\n",
        "  logits = xenc_54 @ W # log-counts\n",
        "  counts = logits.exp() # equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # = softmax\n",
        "\n",
        "  loss = - probs[torch.arange(num), ys].log().mean()\n",
        "  print('Iteration:', k+1, '   Loss:' , loss.item())\n",
        "\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  W.data += -50 * W.grad\n"
      ],
      "metadata": {
        "id": "qngcUQM8O1QT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896c5a88-b209-4aa9-b278-a2d6782415da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1    Loss: 4.232541084289551\n",
            "Iteration: 2    Loss: 3.504002332687378\n",
            "Iteration: 3    Loss: 3.318845510482788\n",
            "Iteration: 4    Loss: 2.957639455795288\n",
            "Iteration: 5    Loss: 2.863344192504883\n",
            "Iteration: 6    Loss: 2.8381974697113037\n",
            "Iteration: 7    Loss: 2.7508206367492676\n",
            "Iteration: 8    Loss: 2.7523350715637207\n",
            "Iteration: 9    Loss: 2.6745989322662354\n",
            "Iteration: 10    Loss: 2.679236888885498\n",
            "Iteration: 11    Loss: 2.626859188079834\n",
            "Iteration: 12    Loss: 2.6468143463134766\n",
            "Iteration: 13    Loss: 2.5863289833068848\n",
            "Iteration: 14    Loss: 2.603274345397949\n",
            "Iteration: 15    Loss: 2.561227321624756\n",
            "Iteration: 16    Loss: 2.589733362197876\n",
            "Iteration: 17    Loss: 2.536099672317505\n",
            "Iteration: 18    Loss: 2.558943510055542\n",
            "Iteration: 19    Loss: 2.5216567516326904\n",
            "Iteration: 20    Loss: 2.5544543266296387\n",
            "Iteration: 21    Loss: 2.503983736038208\n",
            "Iteration: 22    Loss: 2.529757261276245\n",
            "Iteration: 23    Loss: 2.4951326847076416\n",
            "Iteration: 24    Loss: 2.530426502227783\n",
            "Iteration: 25    Loss: 2.481489896774292\n",
            "Iteration: 26    Loss: 2.5089097023010254\n",
            "Iteration: 27    Loss: 2.476031541824341\n",
            "Iteration: 28    Loss: 2.513054847717285\n",
            "Iteration: 29    Loss: 2.464789867401123\n",
            "Iteration: 30    Loss: 2.493180990219116\n",
            "Iteration: 31    Loss: 2.461641311645508\n",
            "Iteration: 32    Loss: 2.5000154972076416\n",
            "Iteration: 33    Loss: 2.45188307762146\n",
            "Iteration: 34    Loss: 2.4808125495910645\n",
            "Iteration: 35    Loss: 2.450448751449585\n",
            "Iteration: 36    Loss: 2.4899725914001465\n",
            "Iteration: 37    Loss: 2.4416000843048096\n",
            "Iteration: 38    Loss: 2.4707608222961426\n",
            "Iteration: 39    Loss: 2.4415345191955566\n",
            "Iteration: 40    Loss: 2.4820995330810547\n",
            "Iteration: 41    Loss: 2.433208703994751\n",
            "Iteration: 42    Loss: 2.4623634815216064\n",
            "Iteration: 43    Loss: 2.434305191040039\n",
            "Iteration: 44    Loss: 2.475858449935913\n",
            "Iteration: 45    Loss: 2.4262237548828125\n",
            "Iteration: 46    Loss: 2.4551799297332764\n",
            "Iteration: 47    Loss: 2.4283549785614014\n",
            "Iteration: 48    Loss: 2.470872163772583\n",
            "Iteration: 49    Loss: 2.42031192779541\n",
            "Iteration: 50    Loss: 2.4489126205444336\n",
            "Iteration: 51    Loss: 2.423396587371826\n",
            "Iteration: 52    Loss: 2.466865062713623\n",
            "Iteration: 53    Loss: 2.41523814201355\n",
            "Iteration: 54    Loss: 2.443358898162842\n",
            "Iteration: 55    Loss: 2.4192147254943848\n",
            "Iteration: 56    Loss: 2.4636220932006836\n",
            "Iteration: 57    Loss: 2.4108316898345947\n",
            "Iteration: 58    Loss: 2.4383835792541504\n",
            "Iteration: 59    Loss: 2.4156455993652344\n",
            "Iteration: 60    Loss: 2.460965871810913\n",
            "Iteration: 61    Loss: 2.4069695472717285\n",
            "Iteration: 62    Loss: 2.4339029788970947\n",
            "Iteration: 63    Loss: 2.412562370300293\n",
            "Iteration: 64    Loss: 2.4587535858154297\n",
            "Iteration: 65    Loss: 2.4035608768463135\n",
            "Iteration: 66    Loss: 2.429863929748535\n",
            "Iteration: 67    Loss: 2.4098658561706543\n",
            "Iteration: 68    Loss: 2.4568662643432617\n",
            "Iteration: 69    Loss: 2.400538444519043\n",
            "Iteration: 70    Loss: 2.4262359142303467\n",
            "Iteration: 71    Loss: 2.407478094100952\n",
            "Iteration: 72    Loss: 2.4552035331726074\n",
            "Iteration: 73    Loss: 2.3978521823883057\n",
            "Iteration: 74    Loss: 2.4229984283447266\n",
            "Iteration: 75    Loss: 2.4053397178649902\n",
            "Iteration: 76    Loss: 2.453690767288208\n",
            "Iteration: 77    Loss: 2.3954594135284424\n",
            "Iteration: 78    Loss: 2.420133352279663\n",
            "Iteration: 79    Loss: 2.403407335281372\n",
            "Iteration: 80    Loss: 2.4522786140441895\n",
            "Iteration: 81    Loss: 2.393324375152588\n",
            "Iteration: 82    Loss: 2.4176108837127686\n",
            "Iteration: 83    Loss: 2.401646852493286\n",
            "Iteration: 84    Loss: 2.450937032699585\n",
            "Iteration: 85    Loss: 2.3914144039154053\n",
            "Iteration: 86    Loss: 2.415396213531494\n",
            "Iteration: 87    Loss: 2.400036096572876\n",
            "Iteration: 88    Loss: 2.4496538639068604\n",
            "Iteration: 89    Loss: 2.3896982669830322\n",
            "Iteration: 90    Loss: 2.4134509563446045\n",
            "Iteration: 91    Loss: 2.398554563522339\n",
            "Iteration: 92    Loss: 2.4484214782714844\n",
            "Iteration: 93    Loss: 2.388150453567505\n",
            "Iteration: 94    Loss: 2.4117352962493896\n",
            "Iteration: 95    Loss: 2.3971877098083496\n",
            "Iteration: 96    Loss: 2.4472391605377197\n",
            "Iteration: 97    Loss: 2.3867475986480713\n",
            "Iteration: 98    Loss: 2.4102160930633545\n",
            "Iteration: 99    Loss: 2.395923614501953\n",
            "Iteration: 100    Loss: 2.4461095333099365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample the NN"
      ],
      "metadata": {
        "id": "NukFJgWPiFkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5FuGR3Z-GRH",
        "outputId": "a579fc62-cb03-45a8-ccd3-b68642e6d362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "  out = []\n",
        "  ix = [0,0]\n",
        "  while True:\n",
        "    xenc = F.one_hot(torch.tensor(ix), num_classes=27).float()\n",
        "    z = xenc.shape[0]\n",
        "    a = xenc[0].tolist()\n",
        "    b = xenc[1].tolist()\n",
        "    ab = a + b\n",
        "    xenc_54 = torch.tensor(ab)\n",
        "\n",
        "\n",
        "    logits = xenc_54 @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    p = counts/counts.sum(0, keepdims=True) # probabilities for next character\n",
        "\n",
        "    ix_b = torch.multinomial(p, num_samples=1, replacement=True, generator=g)\n",
        "    ix_int =int(ix_b)\n",
        "    \"\"\"\n",
        "    Careful! I had problems here, since itos[ix] did not work.\n",
        "    I cannot call upon a dict with a tensor object!\n",
        "    \"\"\"\n",
        "    out.append(itos[ix_int])\n",
        "\n",
        "    \"\"\"\n",
        "    Now I have to construct my next input.\n",
        "    The NN with just 1 input was easy, I could just use the output. Here I have to build my next input.\n",
        "    \"\"\"\n",
        "\n",
        "    ix_a = int(ix[1])\n",
        "    ix_b = ix_int\n",
        "\n",
        "    if ix_int == 0:\n",
        "      break\n",
        "    else: # We only have to build the next input, if we don't didn't get a end-signal.\n",
        "      ix = [ix_a, ix_b]\n",
        "\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd15fcTjiKtD",
        "outputId": "bb8f3310-ccc2-4e03-a1db-dcb7ff1a5498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "juwide.\n",
            "janasaz.\n",
            "pariay.\n",
            "ainn.\n",
            "kai.\n",
            "ritoleras.\n",
            "the.\n",
            "adannaauranileniassibdainrwi.\n",
            "ta.\n",
            "saisiely.\n",
            "arte.\n",
            "farmumarif.\n",
            "dammer.\n",
            "fyns.\n",
            "jah.\n",
            "jayl.\n",
            "core.\n",
            "yaenockay.\n",
            "aabdin.\n",
            "jamikim.\n",
            "ynil.\n",
            "anaasnassoimonselhadgon.\n",
            "mattanil.\n",
            "arie.\n",
            "alethat.\n",
            "jayreli.\n",
            "iah.\n",
            "dyn.\n",
            "rajemur.\n",
            "mah.\n",
            "dawyle.\n",
            "kay.\n",
            "caarr.\n",
            "janh.\n",
            "adorta.\n",
            "malynabel.\n",
            "ha.\n",
            "ialavarocbethemielya.\n",
            "ar.\n",
            "fa.\n",
            "sely.\n",
            "tavilatikyasaloe.\n",
            "marnandewkfabjanelah.\n",
            "ahen.\n",
            "ch.\n",
            "kaura.\n",
            "odridrdzerlialiyphrgha.\n",
            "tazrwen.\n",
            "aa.\n",
            "vawh.\n",
            "elvan.\n",
            "haviobdandel.\n",
            "gmil.\n",
            "brasabdoni.\n",
            "pexania.\n",
            "arenn.\n",
            "jaylandais.\n",
            "caremyn.\n",
            "mariannjed.\n",
            "adomarelli.\n",
            "deriganden.\n",
            "dan.\n",
            "vadon.\n",
            "basstentti.\n",
            "abus.\n",
            "dasa.\n",
            "la.\n",
            "tan.\n",
            "codfina.\n",
            "asdoshatalieondeslef.\n",
            "dariv.\n",
            "mashama.\n",
            "va.\n",
            "hannge.\n",
            "jardiynivion.\n",
            "urien.\n",
            "jalivnal.\n",
            "alle.\n",
            "marafjlanlynialey.\n",
            "maritriyl.\n",
            "arsoiredertiparkenniyaneria.\n",
            "ahul.\n",
            "la.\n",
            "acvatavryael.\n",
            "sabrla.\n",
            "sa.\n",
            "puralialieri.\n",
            "kazinarahhiha.\n",
            "ia.\n",
            "jasaileorvann.\n",
            "audcelaz.\n",
            "arieertiliahadayra.\n",
            "fbrlya.\n",
            "caeyton.\n",
            "za.\n",
            "sabran.\n",
            "ardeve.\n",
            "zasslen.\n",
            "conak.\n",
            "na.\n"
          ]
        }
      ]
    }
  ]
}